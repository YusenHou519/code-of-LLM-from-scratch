{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x23e8167cc10>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\" \n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "with open(\"./the-verdict.txt\",'r',encoding=\"utf-8\") as f:\n",
    "    raw_data = f.read()\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_data)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens and Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# 构建词汇表\n",
    "vocab = {token:integer for integer,token in enumerate(set(preprocessed))}\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimapleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str2int = vocab\n",
    "        self.int2str = {i:k for k,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, raw):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.split()]\n",
    "        ids = [self.str2int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        texts = \" \".join([self.int2str[i] for i in ids])\n",
    "        texts = re.sub(r'\\s+([,.?!\"()\\'])',r'\\1', texts)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1106, 283, 564, 614, 686]\n",
      "It' s me!\n"
     ]
    }
   ],
   "source": [
    "t1 = SimapleTokenizerV1(vocab=vocab)\n",
    "ids = t1.encode(\"It's me!\")\n",
    "print(ids)\n",
    "print(t1.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "# 添加特殊字符\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|unk|>\",\"<|endoftext|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新 SimapleTokenizerV1 \n",
    "class SimapleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str2int = vocab\n",
    "        self.int2str = {i:k for k,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, texts):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', texts)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str2int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str2int[s] for s in preprocessed]\n",
    "        return ids \n",
    "    \n",
    "    def decode(self, ids):\n",
    "        texts = \" \".join([self.int2str[i] for i in ids])\n",
    "        texts = re.sub(r'\\s+([,.?\"!_):;\\'])',r'\\1',texts)\n",
    "        texts = re.sub(r'([(])+\\s',r'\\1',texts)\n",
    "        return texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 999, 584, 8, 663, 9, 14, 987, 584, 3, 533, 4, 7]\n",
      "<|unk|>, this is: me; And that is (he).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimapleTokenizerV2(vocab)\n",
    "texts = \"Hello, this is: me; And that is (he).\"\n",
    "ids = tokenizer.encode(texts)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dc/da/8d1cc3089a83f5cf11c2e489332752981435280285231924557350523a59/tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "     ---------------------------------------- 0.0/884.2 kB ? eta -:--:--\n",
      "     ----------- ---------------------------- 262.1/884.2 kB ? eta -:--:--\n",
      "     --------------------------------- ---- 786.4/884.2 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 884.2/884.2 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/42/7e/5f1b92c8468290c465fd50c5318da64319133231415a8aa6ea5ab995a815/regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/20/f1d4670a8a723c46be695dff449d86d6092916f9e99c53051954ee33a1bc/charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/12/90/3c9ff0512038035f59d279fddeb79f5f1eccd8859f06d6163c58798b9487/certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Installing collected packages: urllib3, regex, idna, charset-normalizer, certifi, requests, tiktoken\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 regex-2024.11.6 requests-2.32.3 tiktoken-0.8.0 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\" \n",
    "        \"of someunknownPlace.\" )\n",
    "print(tokenizer.encode(text,allowed_special={\"<|endoftext|>\"}))\n",
    "print(tokenizer.decode(tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = \"Akwirw ier\"\n",
    "print(tokenizer.encode(text,allowed_special={\"<|endoftext|>\"}))\n",
    "print(tokenizer.decode(tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建 dataset 和 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input = []\n",
    "        self.target = []\n",
    "        ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0,len(ids)-max_length,stride):\n",
    "            self.input.append(torch.tensor(ids[i:i+max_length]))\n",
    "            self.target.append(torch.tensor(ids[i+1:i+max_length+1]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.target[idx]\n",
    "    \n",
    "def create_dataloader_v1(text, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "                            num_workers=num_workers, drop_last=drop_last)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[10724,   262,  6846,   338]]), tensor([[  262,  6846,   338, 11428]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    raw_text = f.read()\n",
    "t_dataloader=create_dataloader_v1(raw_text,batch_size=1,max_length=4,stride=1)\n",
    "t_dataloader=iter(t_dataloader)\n",
    "print(next(t_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1115,  0.1204],\n",
      "        [-0.3696, -0.2404],\n",
      "        [-1.1969,  0.2093],\n",
      "        [-0.9724, -0.7550],\n",
      "        [ 0.3239, -0.1085]], requires_grad=True)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "vocab_size=5\n",
    "output_dim=2\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) \n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(torch.tensor([3,4])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 增加 positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n",
      "embedding.shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "max_length=4\n",
    "dataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=max_length,stride=max_length,shuffle=False)\n",
    "data_iter=iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(inputs.shape)\n",
    "print(targets.shape)\n",
    "vocab_size = 50257 \n",
    "output_dim = 256 \n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "positional_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "positional_embedding = positional_embedding_layer(torch.arange(max_length))\n",
    "print(positional_embedding.shape)\n",
    "embedding = token_embeddings + positional_embedding\n",
    "print(f\"embedding.shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
